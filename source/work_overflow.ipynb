{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T06:42:02.138774Z",
     "start_time": "2025-05-15T06:42:02.133747Z"
    }
   },
   "source": [
    "from utils.util import *\n",
    "from ultralytics import RTDETR\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T06:42:06.222419Z",
     "start_time": "2025-05-15T06:42:05.739754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "detector = RTDETR(\"../models/object_detection.pt\")\n",
    "\n",
    "classifier = get_pretrained_resnet(num_classes=1, pretrained=False)\n",
    "classifier.load_state_dict(torch.load(\"../models/defect_detection_model.pth\"))\n",
    "classifier.to(DEVICE)\n",
    "\n",
    "detectable_classes = {7:0, 5:1, 11:2, 1:4}\n",
    "class_to_model = {0 : \"defect_detection_glass_model.pth\", 1 : \"defect_detection_lightning_model.pth\", 2: \"defect_detection_polymer_model.pth\", 4: \"defect_detection_yoke_model.pth\"}\n",
    "class_to_problem = {0 : \"Missing cap\", 1 : \"Rust\", 2: \"Rust\", 4: \"Rust\"}"
   ],
   "id": "9c0b7e75c17544ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 loaded. Final layer replaced for 1 output features.\n",
      "Only the final layer will be trained initially.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T06:42:08.157941Z",
     "start_time": "2025-05-15T06:42:08.154557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model(class_id):\n",
    "    model_path = class_to_model[class_id]\n",
    "    model_path = os.path.join(\"../models\", model_path)\n",
    "    model = get_pretrained_resnet(num_classes=1, pretrained=False)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ],
   "id": "236888c92cba9929",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:38:25.861456Z",
     "start_time": "2025-05-15T07:38:25.854092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_full_pipeline(img, image_path:str|Path, save_dir:Path|None=None, pad:int=0):\n",
    "    \"\"\"\n",
    "    • Detect parts with REDETR\n",
    "    • Crop each part\n",
    "    • Run defect classifier on the crops\n",
    "    • Optionally save crops for inspection\n",
    "    \"\"\"\n",
    "    original_img = img\n",
    "\n",
    "    if img is None:\n",
    "        original_img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    img_defections   = original_img.copy()          # red boxes will go here\n",
    "    draw    = ImageDraw.Draw(img_defections)\n",
    "\n",
    "    results = detector.predict(original_img)\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        print(f\"  Type of result.boxes: {type(result.boxes)}\")\n",
    "        print(f\"  Value of result.boxes: {result.boxes}\")\n",
    "        if result.boxes is not None and len(result.boxes) > 0:\n",
    "            boxes = result.boxes\n",
    "            confidences = boxes.conf\n",
    "            cls_indices = boxes.cls\n",
    "\n",
    "            print(f\"  Type of boxes: {type(boxes)}\")\n",
    "            print(f\"  Type of confidences: {type(confidences)}\")\n",
    "            print(f\"  Type of cls_indices: {type(cls_indices)}\")\n",
    "\n",
    "            for box, confidence, class_id in zip(boxes.xyxy, confidences, cls_indices):\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "                confidence_value = confidence.item()\n",
    "                class_name = result.names[int(class_id)]\n",
    "                print(f\"    Detected {class_name} with confidence: {confidence_value:.2f} at ({x1:.0f}, {y1:.0f}), ({x2:.0f}, {y2:.0f})\")\n",
    "                if class_id in detectable_classes:\n",
    "                    id = detectable_classes[class_id]\n",
    "                    crop = crop_object(original_img, box)\n",
    "                    model = get_model(id)\n",
    "                    prob, label = predict_single(crop, model, DEVICE)\n",
    "                    if label != 0:\n",
    "                        print(f\"    Detected {class_to_problem[id]} with confidence: {prob:.2f}\")\n",
    "                        draw.rectangle([(x1, y1), (x2, y2)], outline=\"red\", width=1)\n",
    "                        draw.text((x1, y1 - 12), class_to_problem[id], fill=\"red\")\n",
    "                        detections.append({\"id\": id, \"prob\": prob, \"label\": label, \"type\": class_to_problem[id], \"box\": box, \"confidence\": confidence})\n",
    "\n",
    "\n",
    "            img_object_detection = Image.fromarray(result.plot().astype('uint8')) # Convert to PIL Image\n",
    "            save_path = f\"./{save_dir}/object_{os.path.basename(image_path)}\"\n",
    "            save_path_d = f\"./{save_dir}/detected_{os.path.basename(image_path)}\"\n",
    "            img_object_detection.save(save_path)  # Now you can use .save()\n",
    "            print(f\"  Saved annotated image to: {save_path}\")\n",
    "            if detections:\n",
    "                img_defections.save(save_path_d)  # Now you can use .save()\n",
    "                print(f\"  Saved defected image to: {save_path}\")\n",
    "            else:\n",
    "                print(\"  No defects detected\")\n",
    "            return result, detections\n",
    "        else:\n",
    "            print(\"  No objects detected in this image.\")"
   ],
   "id": "176481ece009a604",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:38:27.941937Z",
     "start_time": "2025-05-15T07:38:27.265947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_img = '/Users/azizbek/Downloads/tok-stoyka.png'\n",
    "outputs  = run_full_pipeline(img = None, image_path = test_img, save_dir=\"debug_crops\")\n",
    "print(outputs)"
   ],
   "id": "fdaf8067d2aa2f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 yoke, 2 yoke suspensions, 1 stockbridge damper, 1 polymer insulator, 1 polymer insulator lower shackle, 307.8ms\n",
      "Speed: 2.8ms preprocess, 307.8ms inference, 0.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "  Type of result.boxes: <class 'ultralytics.engine.results.Boxes'>\n",
      "  Value of result.boxes: ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 6.,  1.,  1., 10.,  0.,  3.])\n",
      "conf: tensor([0.9585, 0.9519, 0.9504, 0.9173, 0.9640, 0.7293])\n",
      "data: tensor([[5.5177e+02, 2.4817e-01, 1.0419e+03, 2.5341e+02, 9.5853e-01, 6.0000e+00],\n",
      "        [1.5869e+03, 3.0617e+02, 1.7950e+03, 7.5625e+02, 9.5195e-01, 1.0000e+00],\n",
      "        [8.1519e+01, 2.3358e+02, 2.8312e+02, 6.6412e+02, 9.5043e-01, 1.0000e+00],\n",
      "        [6.8174e+02, 2.4752e+02, 9.1425e+02, 5.4791e+02, 9.1726e-01, 1.0000e+01],\n",
      "        [8.3834e+01, 2.3688e+02, 1.7490e+03, 1.0809e+03, 9.6403e-01, 0.0000e+00],\n",
      "        [3.8577e+02, 9.5246e+02, 8.9732e+02, 1.0799e+03, 7.2926e-01, 3.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1080, 1920)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[ 796.8268,  126.8280,  490.1089,  253.1596],\n",
      "        [1690.9812,  531.2087,  208.0994,  450.0741],\n",
      "        [ 182.3207,  448.8490,  201.6039,  430.5424],\n",
      "        [ 797.9985,  397.7117,  232.5071,  300.3896],\n",
      "        [ 916.4355,  658.8705, 1665.2029,  843.9871],\n",
      "        [ 641.5444, 1016.1698,  511.5461,  127.4291]])\n",
      "xywhn: tensor([[0.4150, 0.1174, 0.2553, 0.2344],\n",
      "        [0.8807, 0.4919, 0.1084, 0.4167],\n",
      "        [0.0950, 0.4156, 0.1050, 0.3987],\n",
      "        [0.4156, 0.3683, 0.1211, 0.2781],\n",
      "        [0.4773, 0.6101, 0.8673, 0.7815],\n",
      "        [0.3341, 0.9409, 0.2664, 0.1180]])\n",
      "xyxy: tensor([[5.5177e+02, 2.4817e-01, 1.0419e+03, 2.5341e+02],\n",
      "        [1.5869e+03, 3.0617e+02, 1.7950e+03, 7.5625e+02],\n",
      "        [8.1519e+01, 2.3358e+02, 2.8312e+02, 6.6412e+02],\n",
      "        [6.8174e+02, 2.4752e+02, 9.1425e+02, 5.4791e+02],\n",
      "        [8.3834e+01, 2.3688e+02, 1.7490e+03, 1.0809e+03],\n",
      "        [3.8577e+02, 9.5246e+02, 8.9732e+02, 1.0799e+03]])\n",
      "xyxyn: tensor([[2.8738e-01, 2.2978e-04, 5.4265e-01, 2.3464e-01],\n",
      "        [8.2653e-01, 2.8349e-01, 9.3491e-01, 7.0023e-01],\n",
      "        [4.2458e-02, 2.1628e-01, 1.4746e-01, 6.1493e-01],\n",
      "        [3.5508e-01, 2.2918e-01, 4.7617e-01, 5.0732e-01],\n",
      "        [4.3664e-02, 2.1933e-01, 9.1096e-01, 1.0008e+00],\n",
      "        [2.0092e-01, 8.8190e-01, 4.6735e-01, 9.9989e-01]])\n",
      "  Type of boxes: <class 'ultralytics.engine.results.Boxes'>\n",
      "  Type of confidences: <class 'torch.Tensor'>\n",
      "  Type of cls_indices: <class 'torch.Tensor'>\n",
      "    Detected polymer insulator with confidence: 0.96 at (552, 0), (1042, 253)\n",
      "    Detected yoke suspension with confidence: 0.95 at (1587, 306), (1795, 756)\n",
      "    Detected yoke suspension with confidence: 0.95 at (82, 234), (283, 664)\n",
      "    Detected polymer insulator lower shackle with confidence: 0.92 at (682, 248), (914, 548)\n",
      "    Detected yoke with confidence: 0.96 at (84, 237), (1749, 1081)\n",
      "    Detected stockbridge damper with confidence: 0.73 at (386, 952), (897, 1080)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "conversion from RGB to BRG not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m test_img \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/Users/azizbek/Downloads/tok-stoyka.png\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 2\u001B[0m outputs  \u001B[38;5;241m=\u001B[39m run_full_pipeline(img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, image_path \u001B[38;5;241m=\u001B[39m test_img, save_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdebug_crops\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs)\n",
      "Cell \u001B[0;32mIn[35], line 47\u001B[0m, in \u001B[0;36mrun_full_pipeline\u001B[0;34m(img, image_path, save_dir, pad)\u001B[0m\n\u001B[1;32m     43\u001B[0m             draw\u001B[38;5;241m.\u001B[39mtext((x1, y1 \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m12\u001B[39m), class_to_problem[\u001B[38;5;28mid\u001B[39m], fill\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mred\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     44\u001B[0m             detections\u001B[38;5;241m.\u001B[39mappend({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mid\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprob\u001B[39m\u001B[38;5;124m\"\u001B[39m: prob, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m: label, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: class_to_problem[\u001B[38;5;28mid\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbox\u001B[39m\u001B[38;5;124m\"\u001B[39m: box, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfidence\u001B[39m\u001B[38;5;124m\"\u001B[39m: confidence})\n\u001B[0;32m---> 47\u001B[0m img_object_detection \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(result\u001B[38;5;241m.\u001B[39mplot()\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muint8\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBRG\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;66;03m# Convert to PIL Image\u001B[39;00m\n\u001B[1;32m     48\u001B[0m save_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/object_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(image_path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     49\u001B[0m save_path_d \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/detected_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(image_path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:1068\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m   1065\u001B[0m     dither \u001B[38;5;241m=\u001B[39m Dither\u001B[38;5;241m.\u001B[39mFLOYDSTEINBERG\n\u001B[1;32m   1067\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1068\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mim\u001B[38;5;241m.\u001B[39mconvert(mode, dither)\n\u001B[1;32m   1069\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m   1070\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1071\u001B[0m         \u001B[38;5;66;03m# normalize source image and try again\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: conversion from RGB to BRG not supported"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T10:14:07.132186Z",
     "start_time": "2025-05-14T10:14:07.126200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, random, cv2, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "DEFECT_THR  = 0.5            # show box only if defect‑prob ≥ this\n",
    "PAD         = 4              # pixels of padding when cropping\n",
    "\n",
    "# simple RGB‑to‑tensor preproc matching your classifier\n",
    "def to_tensor(img_bgr):\n",
    "    return ( torch.from_numpy(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "             .permute(2,0,1).float()/255. ).unsqueeze(0)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def detect_and_filter(image_path: str|Path):\n",
    "    im_bgr   = cv2.imread(str(image_path))\n",
    "    h, w     = im_bgr.shape[:2]\n",
    "\n",
    "    # 1 ▸ DETECT PARTS\n",
    "    results  = detector.predict(im_bgr, conf=0.65, device=DEVICE, verbose=False)[0]\n",
    "    if len(results.boxes) == 0:                      # → nothing at all\n",
    "        return im_bgr, []\n",
    "\n",
    "    boxes_xyxy = results.boxes.xyxy.cpu().numpy()    # [N,4]\n",
    "    part_names = [results.names[int(c)] for c in results.boxes.cls]\n",
    "\n",
    "    keep, info = [], []                              # filtered indices & meta\n",
    "    # 2 ▸ CROP ▸ CLASSIFY EACH BOX\n",
    "    for idx, (x1,y1,x2,y2) in enumerate(boxes_xyxy.astype(int)):\n",
    "        # crop with small padding\n",
    "        x1p,y1p = max(x1-PAD,0), max(y1-PAD,0)\n",
    "        x2p,y2p = min(x2+PAD,w-1), min(y2+PAD,h-1)\n",
    "        crop    = im_bgr[y1p:y2p, x1p:x2p]\n",
    "\n",
    "        prob = torch.sigmoid(classifier(to_tensor(crop).to(DEVICE))).item()\n",
    "        if prob >= DEFECT_THR:                       # defective → keep\n",
    "            keep.append(idx)\n",
    "            info.append((x1,y1,x2,y2, part_names[idx], prob))\n",
    "\n",
    "    return im_bgr, info                              # original image + kept boxes\n",
    "\n",
    "# ─── TEST LOOP ────────────────────────────────────────────────────────────────\n"
   ],
   "id": "2f3eceffdd5084cb",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T10:23:57.230836Z",
     "start_time": "2025-05-14T10:23:46.866954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_test_images = 5\n",
    "# Path to your directory of random images\n",
    "image_dir = \"../data/InsPLAD-det/val/images\"\n",
    "image_dir = \"../data/InsPLAD-fault/unsupervised_anomaly_detection/glass-insulator/test/missingcap\"\n",
    "\n",
    "# Get a list of all image files\n",
    "image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "for _ in range(num_test_images):\n",
    "    img_path = random.choice(image_files)\n",
    "    print(f\"\\n--- Processing image: {os.path.basename(img_path)} ---\")\n",
    "\n",
    "    img_bgr, bad_parts = detect_and_filter(img_path)\n",
    "    if not bad_parts:\n",
    "        print(\"  No defects found.\")\n",
    "        continue\n",
    "\n",
    "    # draw only “bad” boxes\n",
    "    for (x1,y1,x2,y2, label, prob) in bad_parts:\n",
    "        cv2.rectangle(img_bgr, (x1,y1), (x2,y2), (0,0,255), 2)          # red box\n",
    "        cv2.putText(img_bgr,\n",
    "                    f\"{label}: {prob:.2f}\",\n",
    "                    (x1, max(y1-5,12)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "        print(f\"    DEFECT {label}  p={prob:.2f}  box=({x1},{y1})–({x2},{y2})\")\n",
    "\n",
    "    save_path = f\"../results/defect_detection/defects_{os.path.basename(img_path)}\"\n",
    "    cv2.imwrite(save_path, img_bgr)\n",
    "    print(f\"  Saved annotated defects‑only image to: {save_path}\")"
   ],
   "id": "6920529b8dd84de5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing image: Fotos 21-10-2020_DJI_0557_cadeia_isolador_vidro_1630.jpg ---\n",
      "  No defects found.\n",
      "\n",
      "--- Processing image: Fotos 21-10-2020_DJI_0537_cadeia_isolador_vidro_1615.jpg ---\n",
      "  No defects found.\n",
      "\n",
      "--- Processing image: Fotos 07-12-2020_DJI_0134_cadeia_isolador_vidro_1338.jpg ---\n",
      "  No defects found.\n",
      "\n",
      "--- Processing image: Fotos 03-12-2020_DJI_0361_cadeia_isolador_vidro_911.jpg ---\n",
      "  No defects found.\n",
      "\n",
      "--- Processing image: Fotos 03-12-2020_DJI_0361_cadeia_isolador_vidro_912.jpg ---\n",
      "  No defects found.\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
